{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a6147b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                                  \n",
    "import numpy as np                                \n",
    "import pandas as pd       \n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt \n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7c15c66",
   "metadata": {},
   "source": [
    "## Content\n",
    "1. Import all raw count data\n",
    "2. Calculate the AADT\n",
    "3. Adding region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2079ff25",
   "metadata": {},
   "source": [
    "### 1. Import all raw count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90af389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations=pd.read_excel('C:/Users/P-Koirala/OneDrive - Texas A&M Transportation Institute/Desktop/Data/Strava_project/#RAW/Data_Preparation/Travis_Station_datayear.xlsx', \n",
    "                       usecols={'Station Id Tmg','Latitude','Longitude'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "138a39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_17={}\n",
    "path='C:/Users/P-Koirala/OneDrive - Texas A&M Transportation Institute/Desktop/Data/Strava_project/#RAW/Data_Preparation/new_count_data_2017_BPCX'\n",
    "for x in os.listdir(path):\n",
    "    loc_number=x[2:-4]\n",
    "    count_17[loc_number]=pd.read_csv(path+'/'+x)\n",
    "#2018\n",
    "count_18={}\n",
    "path='C:/Users/P-Koirala/OneDrive - Texas A&M Transportation Institute/Desktop/Data/Strava_project/#RAW/Data_Preparation/new_count_data_2018_BPCX'\n",
    "for x in os.listdir(path):\n",
    "    loc_number=x[4:-4]\n",
    "    count_18[loc_number]=pd.read_csv(path+'/'+x)\n",
    "#2019\n",
    "count_19={}\n",
    "path='C:/Users/P-Koirala/OneDrive - Texas A&M Transportation Institute/Desktop/Data/Strava_project/#RAW/Data_Preparation/new_count_data_2019_BPCX'\n",
    "for x in os.listdir(path):\n",
    "    loc_number=x[2:-4]\n",
    "    count_19[loc_number]=pd.read_csv(path+'/'+x)\n",
    "#2021 (2020 is excluded because of covid)\n",
    "count_21={}\n",
    "path='C:/Users/P-Koirala/OneDrive - Texas A&M Transportation Institute/Desktop/Data/Strava_project/#RAW/Data_Preparation/new_count_data_2021_BPCX'\n",
    "for x in os.listdir(path):\n",
    "    loc_number=x[2:-4]\n",
    "    count_21[loc_number]=pd.read_csv(path+'/'+x)\n",
    "#2022\n",
    "count_22={}\n",
    "path='C:/Users/P-Koirala/OneDrive - Texas A&M Transportation Institute/Desktop/Data/Strava_project/#RAW/Data_Preparation/new_count_data_2022_BPCX'\n",
    "for x in os.listdir(path):\n",
    "    loc_number=x[2:-4]\n",
    "    count_22[loc_number]=pd.read_csv(path+'/'+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "32649246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed17:  AU0017\n",
      "removed18:  AU0017\n",
      "removed19:  AU0017\n",
      "removed21  AU0017\n",
      "removed21  AU0051\n",
      "removed21  AU0054\n",
      "removed21  AU0057\n",
      "removed21  AU0058\n",
      "removed21  AU0059\n",
      "removed21  AU0060\n",
      "removed21  AU0061\n",
      "removed22  AU0017\n"
     ]
    }
   ],
   "source": [
    "#For 2017\n",
    "for station in list(count_17.keys()):\n",
    "    #print(station)\n",
    "    c=len(count_17[station].Direction.unique())\n",
    "    if c== 4:\n",
    "        count_17.pop(station,None)\n",
    "        print('removed17: ',station)\n",
    "\n",
    "#For 2018\n",
    "for station in list(count_18.keys()):\n",
    "    #print(station)\n",
    "    c=len(count_18[station].Direction.unique())\n",
    "    if c== 4 or c==1:\n",
    "        count_18.pop(station,None)\n",
    "        print('removed18: ',station)\n",
    "        \n",
    "#For 2019\n",
    "for station in list(count_19.keys()):\n",
    "    #print(station)\n",
    "    c=len(count_19[station].Direction.unique())\n",
    "    if c== 4 or c==1:\n",
    "        count_19.pop(station,None)\n",
    "        print('removed19: ',station)\n",
    "\n",
    "#For 2021\n",
    "for station in list(count_21.keys()):\n",
    "    #print(station)\n",
    "    c=len(count_21[station].Direction.unique())\n",
    "    if c== 4:\n",
    "        count_21.pop(station,None)\n",
    "        print('removed21 ', station)\n",
    "#For  2022\n",
    "for station in list(count_22.keys()):\n",
    "    #print(station)\n",
    "    c=len(count_22[station].Direction.unique())\n",
    "    if c== 4:\n",
    "        count_22.pop(station,None)\n",
    "        print('removed22 ', station)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c03ff6",
   "metadata": {},
   "source": [
    "### 2. Calculate AADT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b336f9",
   "metadata": {},
   "source": [
    "#### 2.1 Seperating pedestrian and bicycle counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a58ddbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#seperating pedestrian and bicycle data from 2017 counts\n",
    "count_17_ped={}\n",
    "count_17_bike={}\n",
    "for i in list(count_17.keys()):\n",
    "    df=count_17[i]\n",
    "    if len(df[df[\"Count Type\"]=='Pedestrians only']) != 0:\n",
    "        count_17_ped[i]=df[df[\"Count Type\"]=='Pedestrians only']\n",
    "    if len(df[df[\"Count Type\"]=='Bicycles only']) !=0:\n",
    "        count_17_bike[i]=df[df[\"Count Type\"]=='Bicycles only']\n",
    "#seperating pedestrian and bicycle data from 2018 counts\n",
    "count_18_ped={}\n",
    "count_18_bike={}\n",
    "for i in list(count_18.keys()):\n",
    "    df=count_18[i]\n",
    "    if len(df[df[\"Count Type\"]=='Pedestrians only']) != 0:\n",
    "        count_18_ped[i]=df[df[\"Count Type\"]=='Pedestrians only']\n",
    "    if len(df[df[\"Count Type\"]=='Bicycles only']) !=0:\n",
    "        count_18_bike[i]=df[df[\"Count Type\"]=='Bicycles only']\n",
    "#seperating pedestrian and bicycle data from 2019 counts\n",
    "count_19_ped={}\n",
    "count_19_bike={}\n",
    "for i in list(count_19.keys()):\n",
    "    df=count_19[i]\n",
    "    if len(df[df[\"Count Type\"]=='Pedestrians only']) != 0:\n",
    "        count_19_ped[i]=df[df[\"Count Type\"]=='Pedestrians only']\n",
    "    if len(df[df[\"Count Type\"]=='Bicycles only']) !=0:\n",
    "        count_19_bike[i]=df[df[\"Count Type\"]=='Bicycles only']\n",
    "#seperating pedestrian and bicycle data from 2021 counts\n",
    "count_21_ped={}\n",
    "count_21_bike={}\n",
    "for i in list(count_21.keys()):\n",
    "    df=count_21[i]\n",
    "    if len(df[df[\"Count Type\"]=='Pedestrians only']) != 0:\n",
    "        count_21_ped[i]=df[df[\"Count Type\"]=='Pedestrians only']\n",
    "    if len(df[df[\"Count Type\"]=='Bicycles only']) !=0:\n",
    "        count_21_bike[i]=df[df[\"Count Type\"]=='Bicycles only']\n",
    "    \n",
    "#seperating pedestrian and bicycle data from 2022 counts\n",
    "count_22_ped={}\n",
    "count_22_bike={}\n",
    "for i in list(count_22.keys()):\n",
    "    df=count_22[i]\n",
    "    if len(df[df[\"Count Type\"]=='Pedestrians only']) != 0:\n",
    "        count_22_ped[i]=df[df[\"Count Type\"]=='Pedestrians only']\n",
    "    if len(df[df[\"Count Type\"]=='Bicycles only']) !=0:\n",
    "        count_22_bike[i]=df[df[\"Count Type\"]=='Bicycles only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "125e1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding northbound, southbound counts\n",
    "#Bicycle\n",
    "count_17_bike_agg={}\n",
    "count_18_bike_agg={}\n",
    "count_19_bike_agg={}\n",
    "count_21_bike_agg={}\n",
    "count_22_bike_agg={}\n",
    "for station in list(count_17_bike.keys()):\n",
    "    count_17_bike_agg[station]=count_17_bike[station].groupby(count_17_bike[station]['Day of Date']).agg(sum)\n",
    "for station in list(count_18_bike.keys()):\n",
    "    count_18_bike_agg[station]=count_18_bike[station].groupby(count_18_bike[station]['Day of Date']).agg(sum)\n",
    "for station in list(count_19_bike.keys()):\n",
    "    count_19_bike_agg[station]=count_19_bike[station].groupby(count_19_bike[station]['Day of Date']).agg(sum)\n",
    "for station in list(count_21_bike.keys()):\n",
    "    count_21_bike_agg[station]=count_21_bike[station].groupby(count_21_bike[station]['Day of Date']).agg(sum)\n",
    "for station in list(count_22_bike.keys()):\n",
    "    count_22_bike_agg[station]=count_22_bike[station].groupby(count_22_bike[station]['Day of Date']).agg(sum)\n",
    "    \n",
    "#Pedestrian\n",
    "count_17_ped_agg={}\n",
    "count_18_ped_agg={}\n",
    "count_19_ped_agg={}\n",
    "count_21_ped_agg={}\n",
    "count_22_ped_agg={}\n",
    "for station in list(count_17_ped.keys()):\n",
    "    count_17_ped_agg[station]=count_17_ped[station].groupby(count_17_ped[station]['Day of Date']).agg(sum)\n",
    "for station in list(count_18_ped.keys()):\n",
    "    count_18_ped_agg[station]=count_18_ped[station].groupby(count_18_ped[station]['Day of Date']).agg(sum)\n",
    "for station in list(count_19_ped.keys()):\n",
    "    count_19_ped_agg[station]=count_19_ped[station].groupby(count_19_ped[station]['Day of Date']).agg(sum)\n",
    "for station in list(count_21_ped.keys()):\n",
    "    count_21_ped_agg[station]=count_21_ped[station].groupby(count_21_ped[station]['Day of Date']).agg(sum)\n",
    "for station in list(count_22_ped.keys()):\n",
    "    count_22_ped_agg[station]=count_22_ped[station].groupby(count_22_ped[station]['Day of Date']).agg(sum)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae80889c",
   "metadata": {},
   "source": [
    "#### 2.2 Calculating Factors\n",
    "Permament count stations are used to calculate the factors that is later used by temporary count stations to calculate the AADT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "342cf2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2017 ped done\n",
      "['AU0004', 'AU0014', 'AU0016']\n",
      "2018 ped done\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\P-Koirala\\AppData\\Local\\Temp\\ipykernel_20288\\2592531536.py:84: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  factor_22_ped['factor']= [factor_22_ped.iloc[i].sum()/factor_22_ped.iloc[i].notna().sum() for i in range(365)] #Summing all the factors and dividing by the number of non-Null entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 ped done\n",
      "2017 bike done\n",
      "6\n",
      "10\n",
      "14\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\P-Koirala\\AppData\\Local\\Temp\\ipykernel_20288\\2592531536.py:169: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  factor_22_bike['factor']= [factor_22_bike.iloc[i].sum()/factor_22_bike.iloc[i].notna().sum() for i in range(365)] #Summing all the factors and dividing by the number of non-Null entries\n"
     ]
    }
   ],
   "source": [
    "#FACTORS 2017 Pedestrian #Only one permanent station\n",
    "factor_17_ped = pd.DataFrame( index=pd.date_range(start='2017-01-01', periods=365)) #new dataframe to store all the factors\n",
    "perm_station=[] #list of permanent stations\n",
    "for station in list(count_17_ped.keys()):\n",
    "    count_17_ped_agg[station].index = pd.to_datetime(count_17_ped_agg[station].index)\n",
    "    if len(count_17_ped_agg[station]) >300:\n",
    "        perm_station.append(station)\n",
    "print(len(perm_station))\n",
    "for station in perm_station:\n",
    "    AAD=count_17_ped_agg[station]['Count'].sum()/len(count_17_ped_agg[station])\n",
    "    a=count_17_ped_agg[station]['Count']/AAD #factors\n",
    "    factor_17_ped=pd.concat([factor_17_ped,a],axis=1) #append the factors\n",
    "# Average Factor\n",
    "factor_17_ped['factor']= [factor_17_ped.iloc[i].sum()/factor_17_ped.iloc[i].notna().sum() for i in range(365)] #Summing all the factors and dividing by the number of non-Null entries\n",
    "print('2017 ped done')\n",
    "\n",
    "##########################\n",
    "#FACTORS 2018 Pedestrian\n",
    "factor_18_ped = pd.DataFrame( index=pd.date_range(start='2018-01-01', periods=365)) #new dataframe to store all the factors\n",
    "perm_station=[] #list of permanent stations\n",
    "for station in list(count_18_ped.keys()):\n",
    "    count_18_ped_agg[station].index = pd.to_datetime(count_18_ped_agg[station].index)\n",
    "    if len(count_18_ped_agg[station]) >330:\n",
    "        perm_station.append(station)\n",
    "print(perm_station)\n",
    "for station in perm_station:\n",
    "    AAD=count_18_ped_agg[station]['Count'].sum()/len(count_18_ped_agg[station])\n",
    "\n",
    "    a=count_18_ped_agg[station]['Count']/AAD #factors\n",
    "\n",
    "    factor_18_ped=pd.concat([factor_18_ped,a],axis=1) #append the factors\n",
    "# Average Factor\n",
    "factor_18_ped['factor']= [factor_18_ped.iloc[i].sum()/factor_18_ped.iloc[i].notna().sum() for i in range(365)] #Summing all the factors and dividing by the number of non-Null entries\n",
    "print('2018 ped done')\n",
    "##########################\n",
    "\n",
    "#FACTORS 2019 Pedestrian\n",
    "factor_19_ped = pd.DataFrame( index=pd.date_range(start='2019-01-01', periods=365)) #new dataframe to store all the factors\n",
    "perm_station=[] #list of permanent stations\n",
    "for station in list(count_19_ped.keys()):\n",
    "    count_19_ped_agg[station].index = pd.to_datetime(count_19_ped_agg[station].index)\n",
    "    if len(count_19_ped_agg[station]) >330:\n",
    "        perm_station.append(station)\n",
    "print(len(perm_station))\n",
    "for station in perm_station:\n",
    "    AAD=count_19_ped_agg[station]['Count'].sum()/len(count_19_ped_agg[station])\n",
    "    a=count_19_ped_agg[station]['Count']/AAD #factors\n",
    "    factor_19_ped=pd.concat([factor_19_ped,a],axis=1) #append the factors\n",
    "# Average Factor\n",
    "factor_19_ped['factor']= [factor_19_ped.iloc[i].sum()/factor_19_ped.iloc[i].notna().sum() for i in range(365)] #Summing all the factors and dividing by the number of non-Null entries\n",
    "\n",
    "\n",
    "##########################\n",
    "#FACTORS 2021 Pedestrian\n",
    "factor_21_ped = pd.DataFrame( index=pd.date_range(start='2021-01-01', periods=365)) #new dataframe to store all the factors\n",
    "perm_station=[] #list of permanent stations\n",
    "for station in list(count_21_ped.keys()):\n",
    "    count_21_ped_agg[station].index = pd.to_datetime(count_21_ped_agg[station].index)\n",
    "    if len(count_21_ped_agg[station]) >330:\n",
    "        perm_station.append(station)\n",
    "\n",
    "for station in perm_station:\n",
    "    AAD=count_21_ped_agg[station]['Count'].sum()/len(count_21_ped_agg[station])\n",
    "    a=count_21_ped_agg[station]['Count']/AAD #factors\n",
    "    factor_21_ped=pd.concat([factor_21_ped,a],axis=1) #append the factors\n",
    "# Average Factor\n",
    "factor_21_ped['factor']= [factor_21_ped.iloc[i].sum()/factor_21_ped.iloc[i].notna().sum() for i in range(365)] #Summing all the factors and dividing by the number of non-Null entries\n",
    "\n",
    "\n",
    "##########################\n",
    "#FACTORS 2022 Pedestrian\n",
    "factor_22_ped = pd.DataFrame( index=pd.date_range(start='2022-01-01', periods=365)) #new dataframe to store all the factors\n",
    "perm_station=[] #list of permanent stations\n",
    "for station in list(count_22_ped.keys()):\n",
    "    count_22_ped_agg[station].index = pd.to_datetime(count_22_ped_agg[station].index)\n",
    "    if len(count_22_ped_agg[station]) >150:\n",
    "        perm_station.append(station)\n",
    "\n",
    "for station in perm_station:\n",
    "    AAD=count_22_ped_agg[station]['Count'].sum()/len(count_22_ped_agg[station])\n",
    "    a=count_22_ped_agg[station]['Count']/AAD #ftors\n",
    "    factor_22_ped=pd.concat([factor_22_ped,a],axis=1) #append the factors\n",
    "# Average Factor\n",
    "factor_22_ped['factor']= [factor_22_ped.iloc[i].sum()/factor_22_ped.iloc[i].notna().sum() for i in range(365)] #Summing all the factors and dividing by the number of non-Null entries\n",
    "################################################################################################################\n",
    "print('2022 ped done')\n",
    "##########################\n",
    "#FACTORS 2017 Bike\n",
    "factor_17_bike = pd.DataFrame( index=pd.date_range(start='2017-01-01', periods=365)) #new dataframe to store all the factors\n",
    "perm_station=[] #list of permanent stations\n",
    "for station in list(count_17_bike.keys()):\n",
    "    count_17_bike_agg[station].index = pd.to_datetime(count_17_bike_agg[station].index)\n",
    "    if len(count_17_bike_agg[station]) >330:\n",
    "        perm_station.append(station)\n",
    "\n",
    "for station in perm_station:\n",
    "    AAD=count_17_bike_agg[station]['Count'].sum()/len(count_17_bike_agg[station])\n",
    "    a=count_17_bike_agg[station]['Count']/AAD #factors\n",
    "    factor_17_bike=pd.concat([factor_17_bike,a],axis=1) #append the factors\n",
    "# Average Factor\n",
    "factor_17_bike['factor']= [factor_17_bike.iloc[i].sum()/factor_17_bike.iloc[i].notna().sum() for i in range(365)] #Summing all the factors and dividing by the number of non-Null entries\n",
    "\n",
    "print('2017 bike done')\n",
    "##########################\n",
    "#FACTORS 2018 Bike\n",
    "factor_18_bike = pd.DataFrame( index=pd.date_range(start='2018-01-01', periods=365)) #new dataframe to store all the factors\n",
    "perm_station=[] #list of permanent stations\n",
    "for station in list(count_18_bike.keys()):\n",
    "    count_18_bike_agg[station].index = pd.to_datetime(count_18_bike_agg[station].index)\n",
    "    if len(count_18_bike_agg[station]) >330:\n",
    "        perm_station.append(station)\n",
    "print(len(perm_station))\n",
    "for station in perm_station:\n",
    "    AAD=count_18_bike_agg[station]['Count'].sum()/len(count_18_bike_agg[station])\n",
    "    a=count_18_bike_agg[station]['Count'] #factors\n",
    "    factor_18_bike=pd.concat([factor_18_bike,a],axis=1) #append the factors\n",
    "# Average Factor\n",
    "factor_18_bike['factor']= [factor_18_bike.iloc[i].sum()/factor_18_bike.iloc[i].notna().sum() for i in range(365)] #Summing all the factors and dividing by the number of non-Null entries\n",
    "\n",
    "\n",
    "##########################\n",
    "#FACTORS 2019 Bike\n",
    "factor_19_bike = pd.DataFrame( index=pd.date_range(start='2019-01-01', periods=365)) #new dataframe to store all the factors\n",
    "perm_station=[] #list of permanent stations\n",
    "for station in list(count_19_bike.keys()):\n",
    "    count_19_bike_agg[station].index = pd.to_datetime(count_19_bike_agg[station].index)\n",
    "    if len(count_19_bike_agg[station]) >150:\n",
    "        perm_station.append(station)\n",
    "print(len(perm_station))\n",
    "for station in perm_station:\n",
    "    AAD=count_19_bike_agg[station]['Count'].sum()/len(count_19_bike_agg[station])\n",
    "    a=count_19_bike_agg[station]['Count']/AAD #factors\n",
    "    factor_19_bike=pd.concat([factor_19_bike,a],axis=1) #append the factors\n",
    "# Average Factor\n",
    "factor_19_bike['factor']= [factor_19_bike.iloc[i].sum()/factor_19_bike.iloc[i].notna().sum() for i in range(365)] #Summing all the factors and dividing by the number of non-Null entries\n",
    "\n",
    "\n",
    "##########################\n",
    "#FACTORS 2021 Bike\n",
    "factor_21_bike = pd.DataFrame( index=pd.date_range(start='2021-01-01', periods=365)) #new dataframe to store all the factors\n",
    "perm_station=[] #list of permanent stations\n",
    "for station in list(count_21_bike.keys()):\n",
    "    count_21_bike_agg[station].index = pd.to_datetime(count_21_bike_agg[station].index)\n",
    "    if len(count_21_bike_agg[station]) >330:\n",
    "        perm_station.append(station)\n",
    "print(len(perm_station))\n",
    "for station in perm_station:\n",
    "    AAD=count_21_bike_agg[station]['Count'].sum()/len(count_21_bike_agg[station])\n",
    "    a=count_21_bike_agg[station]['Count']/AAD #factors\n",
    "    factor_21_bike=pd.concat([factor_21_bike,a],axis=1) #append the factors\n",
    "# Average Factor\n",
    "factor_21_bike['factor']= [factor_21_bike.iloc[i].sum()/factor_21_bike.iloc[i].notna().sum() for i in range(365)] #Summing all the factors and dividing by the number of non-Null entries\n",
    "\n",
    "\n",
    "##########################\n",
    "#FACTORS 2022 Bike\n",
    "factor_22_bike = pd.DataFrame( index=pd.date_range(start='2022-01-01', periods=365)) #new dataframe to store all the factors\n",
    "perm_station=[] #list of permanent stations\n",
    "for station in list(count_22_bike.keys()):\n",
    "    count_22_bike_agg[station].index = pd.to_datetime(count_22_bike_agg[station].index)\n",
    "    if len(count_22_bike_agg[station]) >150:\n",
    "        perm_station.append(station)\n",
    "print(len(perm_station))\n",
    "for station in perm_station:\n",
    "    AAD=count_22_bike_agg[station]['Count'].sum()/len(count_22_bike_agg[station])\n",
    "    a=count_22_bike_agg[station]['Count']/AAD #factors\n",
    "    factor_22_bike=pd.concat([factor_22_bike,a],axis=1) #append the factors\n",
    "# Average Factor\n",
    "factor_22_bike['factor']= [factor_22_bike.iloc[i].sum()/factor_22_bike.iloc[i].notna().sum() for i in range(365)] #Summing all the factors and dividing by the number of non-Null entries\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d0ea2bd",
   "metadata": {},
   "source": [
    "#### 2.3 Calculating AADT using contributing factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "1a6b667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AAD_calc(data, Factor,perm_stat_threshold):\n",
    "    df_AAD=[]\n",
    "    for station in list(data.keys()):\n",
    "        if len(data[station]) >=perm_stat_threshold:\n",
    "            Avg_AAD=data[station].Count.sum()/len(data[station])\n",
    "            dic={\"station\": station,\n",
    "                 \"AAD\":Avg_AAD\n",
    "                }\n",
    "            df_AAD.append(dic)\n",
    "        if len(data[station]) <perm_stat_threshold:\n",
    "            indx=data[station].index\n",
    "            Factor.loc[indx,[\"factor\"]] #Extracting corresponding factors for AAD calculation\n",
    "            sum_AAD=((data[station].Count*Factor.loc[indx,[\"factor\"]].factor)).sum()\n",
    "            Avg_AAD=sum_AAD/len(indx)\n",
    "            dic={\"station\": station,\n",
    "                 \"AAD\":Avg_AAD\n",
    "                }\n",
    "            df_AAD.append(dic)\n",
    "    return(pd.DataFrame(df_AAD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb2d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "0093a115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\P-Koirala\\AppData\\Local\\Temp\\ipykernel_20288\\2718351667.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  Avg_AAD=sum_AAD/len(indx)\n"
     ]
    }
   ],
   "source": [
    "AAD_17_ped=AAD_calc(count_17_ped_agg, Factor=factor_17_ped, perm_stat_threshold=330)\n",
    "AAD_18_ped=AAD_calc(count_18_ped_agg, Factor=factor_18_ped, perm_stat_threshold=330)\n",
    "AAD_19_ped=AAD_calc(count_19_ped_agg, Factor=factor_19_ped, perm_stat_threshold=330)\n",
    "AAD_21_ped=AAD_calc(count_21_ped_agg, Factor=factor_21_ped, perm_stat_threshold=330)\n",
    "AAD_22_ped=AAD_calc(count_22_ped_agg, Factor=factor_22_ped, perm_stat_threshold=150)\n",
    "\n",
    "AAD_17_bike=AAD_calc(count_17_bike_agg, Factor=factor_17_bike, perm_stat_threshold=330)\n",
    "AAD_18_bike=AAD_calc(count_18_bike_agg, Factor=factor_18_bike, perm_stat_threshold=330)\n",
    "AAD_19_bike=AAD_calc(count_19_bike_agg, Factor=factor_19_bike, perm_stat_threshold=330)\n",
    "AAD_21_bike=AAD_calc(count_21_bike_agg, Factor=factor_21_bike, perm_stat_threshold=330)\n",
    "AAD_22_bike=AAD_calc(count_22_bike_agg, Factor=factor_22_bike, perm_stat_threshold=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "ae95e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_coordinates(aad):\n",
    "    stations=pd.read_excel('C:/Users/P-Koirala/OneDrive - Texas A&M Transportation Institute/Desktop/Data/Strava_project/#RAW/Data_Preparation/Travis_Station_datayear.xlsx', \n",
    "                       usecols={'Station Id Tmg','Latitude','Longitude'})\n",
    "    aad=aad.set_index(aad.station,drop=True)\n",
    "    indxx=aad.index\n",
    "    e=stations.set_index(stations['Station Id Tmg'],drop=True)\n",
    "    e=e.loc[indxx,['Latitude','Longitude']]\n",
    "    a=pd.concat([aad,e], axis=1)\n",
    "    return(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "de649b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "AAD_17_ped=join_coordinates(AAD_17_ped)\n",
    "AAD_18_ped=join_coordinates(AAD_18_ped)\n",
    "AAD_19_ped=join_coordinates(AAD_19_ped)\n",
    "AAD_21_ped=join_coordinates(AAD_21_ped)\n",
    "AAD_22_ped=join_coordinates(AAD_22_ped)\n",
    "\n",
    "AAD_17_bike=join_coordinates(AAD_17_bike)\n",
    "AAD_18_bike=join_coordinates(AAD_18_bike)\n",
    "AAD_19_bike=join_coordinates(AAD_19_bike)\n",
    "AAD_21_bike=join_coordinates(AAD_21_bike)\n",
    "AAD_22_bike=join_coordinates(AAD_22_bike)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3ce37",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "756956b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPORT:\n",
    "AAD_17_ped.to_csv('Final_data/AAD_17_ped.csv', index=None)\n",
    "AAD_18_ped.to_csv('Final_data/AAD_18_ped.csv', index=None)\n",
    "AAD_19_ped.to_csv('Final_data/AAD_19_ped.csv', index=None)\n",
    "AAD_21_ped.to_csv('Final_data/AAD_21_ped.csv', index=None)\n",
    "AAD_22_ped.to_csv('Final_data/AAD_22_ped.csv', index=None)\n",
    "AAD_17_bike.to_csv('Final_data/AAD_17_bike.csv', index=None)\n",
    "AAD_18_bike.to_csv('Final_data/AAD_18_bike.csv', index=None)\n",
    "AAD_19_bike.to_csv('Final_data/AAD_19_bike.csv', index=None)\n",
    "AAD_21_bike.to_csv('Final_data/AAD_21_bike.csv', index=None)\n",
    "AAD_22_bike.to_csv('Final_data/AAD_22_bike.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30874a4e",
   "metadata": {},
   "source": [
    "### 3. Adding region column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2fc76e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_regions=gpd.read_file('austin_regions/Export_regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "66ed4dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Geographic 2D CRS: EPSG:4326>\n",
       "Name: WGS 84\n",
       "Axis Info [ellipsoidal]:\n",
       "- Lat[north]: Geodetic latitude (degree)\n",
       "- Lon[east]: Geodetic longitude (degree)\n",
       "Area of Use:\n",
       "- name: World.\n",
       "- bounds: (-180.0, -90.0, 180.0, 90.0)\n",
       "Datum: World Geodetic System 1984 ensemble\n",
       "- Ellipsoid: WGS 84\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austin_regions=austin_regions.loc[:,['geometry','region_no']]\n",
    "austin_regions.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e4d60943",
   "metadata": {},
   "outputs": [],
   "source": [
    "AAD_21_ped_=AAD_21_ped.reset_index(drop=True)\n",
    "AAD_22_ped_=AAD_22_ped.reset_index(drop=True)\n",
    "AAD_21_bike_=AAD_21_bike.reset_index(drop=True)\n",
    "AAD_22_bike_=AAD_22_bike.reset_index(drop=True)\n",
    "#\n",
    "AAD_21_ped_=gpd.GeoDataFrame(AAD_21_ped_, geometry=gpd.points_from_xy(AAD_21_ped_.Longitude, AAD_21_ped_.Latitude), crs='EPSG:4326')\n",
    "AAD_22_ped_=gpd.GeoDataFrame(AAD_22_ped_, geometry=gpd.points_from_xy(AAD_22_ped_.Longitude, AAD_22_ped_.Latitude), crs='EPSG:4326')\n",
    "AAD_21_bike_=gpd.GeoDataFrame(AAD_21_bike_, geometry=gpd.points_from_xy(AAD_21_bike_.Longitude, AAD_21_bike_.Latitude), crs='EPSG:4326')\n",
    "AAD_22_bike_=gpd.GeoDataFrame(AAD_22_bike_, geometry=gpd.points_from_xy(AAD_22_bike_.Longitude, AAD_22_bike_.Latitude), crs='EPSG:4326')\n",
    "#\n",
    "AAD_21_ped_=AAD_21_ped_.sjoin(austin_regions, how='left',predicate='intersects')\n",
    "AAD_22_ped_=AAD_22_ped_.sjoin(austin_regions, how='left',predicate='intersects')\n",
    "AAD_21_bike_=AAD_21_bike_.sjoin(austin_regions, how='left',predicate='intersects')\n",
    "AAD_22_bike_=AAD_22_bike_.sjoin(austin_regions, how='left',predicate='intersects')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c2b26b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99187485",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ac92a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AAD_21_ped_.drop(['index_right'],axis=1, inplace=True)\n",
    "AAD_22_ped_.drop(['index_right'],axis=1, inplace=True)\n",
    "AAD_21_bike_.drop(['index_right'],axis=1, inplace=True)\n",
    "AAD_22_bike_.drop(['index_right'],axis=1, inplace=True)\n",
    "AAD_21_ped_.to_csv('Final_data/AAD_21_ped.csv', index = False)\n",
    "AAD_22_ped_.to_csv('Final_data/AAD_22_ped.csv',index = False)\n",
    "AAD_21_bike_.to_csv('Final_data/AAD_21_bike.csv',index = False)\n",
    "AAD_22_bike_.to_csv('Final_data/AAD_22_bike.csv',index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e01a2168",
   "metadata": {},
   "source": [
    "### 4. Processing Strava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing strava files and reading data and aggregating\n",
    "def strava_aad(year,mode):    \n",
    "    b={}\n",
    "    path='C:/Users/P-Koirala/Desktop/Data/Strava_project/Austin/'+mode\n",
    "    strava_AAD={}\n",
    "    SUM=0\n",
    "    i=0\n",
    "    for x in os.listdir(path):\n",
    "        if mode=='Bike':\n",
    "            year_in_name=x[16:-22]\n",
    "        elif mode == 'Ped':\n",
    "            year_in_name=x[16:-21]\n",
    "        if year_in_name==year:\n",
    "            path_=path+'/'+x\n",
    "            all_files= os.listdir(path_)\n",
    "            csv_files = list(filter(lambda f: f.endswith('.csv'), all_files))[0]\n",
    "            a=pd.read_csv(path_+'/'+csv_files)\n",
    "            #Aggregating and summing \n",
    "            b=a.groupby(a.edge_uid)['total_trip_count'].mean()\n",
    "            SUM+=b\n",
    "            i+=1\n",
    "    aad=SUM/i\n",
    "    print(year +  mode)\n",
    "    return(aad[aad.notna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf421af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating by mode and year\n",
    "strava_bike_2018=strava_aad('2018','Bike')\n",
    "strava_bike_2019=strava_aad('2019','Bike')\n",
    "strava_bike_2020=strava_aad('2020','Bike')\n",
    "strava_bike_2021=strava_aad('2021','Bike')\n",
    "strava_bike_2022=strava_aad('2022','Bike')\n",
    "#Pedestrian\n",
    "strava_ped_2018=strava_aad('2018','Ped')\n",
    "strava_ped_2019=strava_aad('2019','Ped')\n",
    "strava_ped_2020=strava_aad('2020','Ped')\n",
    "strava_ped_2021=strava_aad('2021','Ped')\n",
    "strava_ped_2022=strava_aad('2022','Ped')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d53e80dc",
   "metadata": {},
   "source": [
    "#### 4.1 Joining strava and strava shape file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0487b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/P-Koirala/Desktop/Data/Strava_project/Austin/Bike/all_edges_daily_2022-01-01-2022-03-31_ride/b73462367714b2a731c4fdcfba8a2bb8802b6fad628a6f8a04ca40fc133e3e91-1657900269790.shp'\n",
    "shape=gpd.read_file(path)\n",
    "shape.set_index(shape.edgeUID, drop=True, inplace=True)\n",
    "#\n",
    "strava_list=[strava_bike_2018,strava_bike_2019,strava_bike_2020,strava_bike_2021,strava_bike_2022,strava_ped_2018,strava_ped_2019,strava_ped_2020,strava_ped_2021,strava_ped_2022]\n",
    "strava_list_names=['str_bik_18','str_bik_19','str_bik_20','str_bik_21','str_bik_22','str_ped_18','str_ped_19','str_ped_20','str_ped_21','str_ped_22']\n",
    "for i,strava_data in enumerate(strava_list):\n",
    "    print(i)\n",
    "    shape=pd.concat([shape,strava_data],axis=1)\n",
    "    shape.rename(columns={'total_trip_count':strava_list_names[i]}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3c3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "shape.to_file('Final_data/strava_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5e2e8238e9bec3cc0bedfee14f9d00aa96a93c230049aa63d17573e6b98cf882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
